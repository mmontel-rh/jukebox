import kfp
import kfp.dsl as dsl
from kfp.dsl import (
    component,
    Input,
    Output,
    Dataset,
    Metrics,
    Model,
)
from typing import NamedTuple


@component(
    base_image="tensorflow/tensorflow", packages_to_install=["pandas", "scikit-learn"]
)
def preprocess_data(
    in_data: Input[Dataset],
    train_data: Output[Dataset],
    val_data: Output[Dataset],
    test_data: Output[Dataset],
    scaler: Output[Model],
    label_encoder: Output[Model],
):
    """
    Takes the dataset and preprocesses it to better train on the fraud detection model.
    The preprocessing consists of:
    1. Splitting the dataset into training, validation, and testing.
    2. Creating a scaler which scales down the training dataset. This scaler is saved as an artifact.
    3. Calculates the class weights, which will later be used during the training.
    """

    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import (
        StandardScaler,
        MinMaxScaler,
        OneHotEncoder,
        LabelEncoder,
    )
    from sklearn.utils import class_weight
    import pandas as pd
    import pickle
    import numpy as np
    from typing import NamedTuple
    import tensorflow as tf

    df = pd.read_csv(in_data.path)
    df = df.dropna()
    print(df.head())

    X = df[
        [
            "is_explicit",
            "duration_ms",
            "danceability",
            "energy",
            "key",
            "loudness",
            "mode",
            "speechiness",
            "acousticness",
            "instrumentalness",
            "liveness",
            "valence",
            "tempo",
        ]
    ]
    y = df["country"]

    label_encoder_ = LabelEncoder()
    y_encoded = label_encoder_.fit_transform(y)
    y_one_hot = tf.keras.utils.to_categorical(y_encoded)

    # Split the data into training and testing sets so you have something to test the trained model with.
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_one_hot, test_size=0.2, shuffle=False
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.2, stratify=y_train
    )

    # Scale the data to remove mean and have unit variance. The data will be between -1 and 1, which makes
    # it a lot easier for the model to learn than random (and potentially large) values.
    # It is important to only fit the scaler to the training data, otherwise you are leaking information about
    # the global distribution of variables (which is influenced by the test set) into the training set.
    scaler_ = MinMaxScaler()
    X_train = scaler_.fit_transform(X_train.values)

    train_data.path += ".pkl"
    val_data.path += ".pkl"
    test_data.path += ".pkl"
    scaler.path += ".pkl"
    label_encoder.path += ".pkl"

    with open(train_data.path, "wb") as handle:
        pickle.dump((X_train, y_train), handle)
    with open(val_data.path, "wb") as handle:
        pickle.dump((X_val, y_val), handle)
    with open(test_data.path, "wb") as handle:
        pickle.dump((X_test, y_test), handle)
    with open(scaler.path, "wb") as handle:
        pickle.dump(scaler_, handle)
    with open(label_encoder.path, "wb") as handle:
        pickle.dump(label_encoder_, handle)
